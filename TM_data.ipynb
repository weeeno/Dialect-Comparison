{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b401623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, chinese_converter, requests\n",
    "from glob import glob\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text loading\n",
    "start_page = requests.get(\"http://spokentaiwanmandarin.nccu.edu.tw/corpus-data.html\")\n",
    "tree0 = html.fromstring(start_page.content)\n",
    "corpus_pages = tree0.xpath(\"//a[@class='mod-articles-category-title ' and @href]/@href\") #find the link destination\n",
    "\n",
    "def scraper(url, corpus):\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    p_node = tree.xpath(\"//td[@style='width: 770px;']/p\")\n",
    "    #A consistent and unique feature that marks the field of speech transcription texts is the format of the cell. Therefore, use xpath to find the node with an attribute that specifies the target cell. \n",
    "    if p_node:\n",
    "        dialogue = tree.xpath(\"//td[@style='width: 770px;']/p/text()\")\n",
    "    else:\n",
    "        dialogue = tree.xpath(\"//td[@style='width: 770px;']/text()\") #For some reason, the last sample T050 has one missing layer /p. This is to go around the variance.\n",
    "    corpus += dialogue\n",
    "    return\n",
    "\n",
    "total_corpus = []\n",
    "for page in corpus_pages: \n",
    "    scraper(\"http://spokentaiwanmandarin.nccu.edu.tw\"+page, total_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving original text locally for future use if needed\n",
    "with open(\"NCCU_spoken_mandarin_corpus.txt\", \"w\") as f: #saving corpus to local file\n",
    "    for sent in total_corpus:\n",
    "        f.write(sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75458412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process corpus texts\n",
    "#\"hon\" may be used as sentence end particles and is therefore preserved\n",
    "processed_corpus = []\n",
    "extend_count = 0 #syllable extension count--save for future projects: may be related to TM dialect prosody pattern\n",
    "LN_lst = [] #foreign language use to track on the side: any super high fx that can match up to the top 500 mandarin char/word?\n",
    "\n",
    "for turn in total_corpus:\n",
    "    de_unicode = unicodedata.normalize(\"NFKD\", turn)\n",
    "    de_zero = re.sub(r\"\\(0?\\)\", \"\", de_unicode) #resolve byte difference issues in chinese characters\n",
    "    if re.search(r\"=+\", de_zero):\n",
    "        extend_count += len(re.findall(r\"=+\", de_zero))\n",
    "    de_extend = re.sub(r\"=+\", \"\", de_zero) #remove syllable extension marks\n",
    "    if re.search(r\"<L[1-6].*L[1-6]>\", de_extend):\n",
    "        code_switch = re.findall(r\"< ?L ?[1-6].*(?:L ?[1-6] ?>)\", de_extend)\n",
    "        LN_lst += code_switch\n",
    "    elif re.search(r\"[a-zA-Z ]+L ?[1-6] ?>\", de_extend):\n",
    "        code_switch = re.findall(r\"[a-zA-Z ]+L ?[1-6] ?>\", de_extend)\n",
    "        LN_lst += code_switch\n",
    "    de_Ln = re.sub(r\"(?:< ?L ?[1-6].*(?:L ?[1-6] ?>)?)|(?:[a-zA-Z ]+L ?[1-6] ?>)\", \"\", de_extend) #remove code switching marks\n",
    "    delaugh = re.sub(r\"[@＠]+\", \" \", de_Ln) #remove laughing marks\n",
    "    de_bracket = re.sub(r\"[\\[\\]]\", \"\", delaugh) #remove overlapping utterance marks\n",
    "    de_exclaim = re.sub(r\"uh|huh|um|hm|mhm|TSK\", \" \", de_bracket) #may be used as sentence separater #remove nonspeech sounds\n",
    "    de_X = re.sub(r\"< ?[xX]|[xX] ?>\", \"\", de_exclaim) #remove unclear char marks\n",
    "    de_3dots = re.sub(r\"…\", r\"..\", de_X) #resolve another byte difference\n",
    "    shi_variant = re.sub(r\"甚\", \"什\", de_3dots)\n",
    "    ta_variant = re.sub(r\"她\", \"他\", shi_variant)\n",
    "    ni_variant = re.sub(r\"妳\", \"你\", ta_variant) #the last 3 to treat chinese character variants as the same char \n",
    "    sent_marker = re.sub(r\"\\.\\.\\.?| \", \",\", ni_variant) #turn pauses into \",\"\n",
    "    sent_lst = sent_marker.split(\",\") #split text by \",\" into utterances\n",
    "    for sent in sent_lst:\n",
    "        if re.search(r\"[\\u4e00-\\u9fff]+\", sent):\n",
    "            processed_corpus.append(sent) #remove utterances w/o any Chinese characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving raw text locally for future use if needed\n",
    "with open(\"processed_NCCU_corpus.txt\", \"w\") as f: #saving corpus to local file\n",
    "    for sent in processed_corpus:\n",
    "        f.write(sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae943f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dictionary of {ngrams:frequency}\n",
    "def n_dict(n, corpus):\n",
    "    n_dict = {}\n",
    "    for sent in corpus:\n",
    "        sent_len = len(sent) \n",
    "        if sent_len >= n: #skip sentences shorter than n\n",
    "            for num in range(sent_len):\n",
    "                if num+n <= sent_len: #as long as the current n-gram reading window does not exceed the sentence length:\n",
    "                    n_gram = sent[num:num+n] #for each char in the utterance, collect n chars starting from the current char to make n-gram\n",
    "                    if n>2 and n_gram == sent[num]*n: #do not collect an ngram that has 3 or more consecutively repeated chars as these are often interjections\n",
    "                        continue\n",
    "                    else:\n",
    "                        if n_gram in n_dict:\n",
    "                            n_dict[n_gram] +=1\n",
    "                        else:\n",
    "                            n_dict[n_gram] =1\n",
    "                else:\n",
    "                    continue\n",
    "    return n_dict\n",
    "#TM ngram dictionary of {ngram:frequency}:\n",
    "TM_fourdict = n_dict(4, processed_corpus)\n",
    "TM_tridict = n_dict(3, processed_corpus) \n",
    "TM_bidict = n_dict(2, processed_corpus)\n",
    "TM_unidict = n_dict(1, processed_corpus)\n",
    "print(len(TM_tridict), len(TM_bidict), len(TM_unidict)) #for reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dict by frequnecy into tuples\n",
    "def sort_dict2tup(ndict, cutoff):\n",
    "    tup_lst = []\n",
    "    for key in ndict:\n",
    "        if ndict[key] < 5: #skip extremely low frequency ngrams\n",
    "            continue\n",
    "        else:\n",
    "            tup_lst.append((key,ndict[key])) #turn dictionary keys & values into tuple as dictionaries can not be sorted\n",
    "    tup_lst.sort(key = lambda x: x[1], reverse = True) #order list from highest to lowest freqency\n",
    "    return tup_lst[:cutoff] #cut off n-grams after xth item (x=cutoff)\n",
    "\n",
    "#sorted ngram tuples\n",
    "TM_sort_fourtup = sort_dict2tup(TM_fourdict, len(TM_fourdict)) #decided not to use this information in the end\n",
    "TM_sort_tritup = sort_dict2tup(TM_tridict, len(TM_tridict))\n",
    "TM_sort_bitup = sort_dict2tup(TM_bidict, len(TM_bidict))\n",
    "TM_sort_unitup = sort_dict2tup(TM_unidict, len(TM_unidict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24787709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nonword or phrase bigrams\n",
    "TM_top_unigram = [tp[0] for tp in TM_sort_unitup[:7]] #most frequent unigrams that are almost guarantee to be stand-alone real words\n",
    "#['是', '就', '我', '他', '的', '那', '有']\n",
    "TM_except_bigrams = [\"可是\", \"然後\", \"還是\", \"什麼\", \"他們\", \"沒有\", \"還有\"] #bigrams that contains TM_top_unigram--most likely non words or phrases. This can later be compiled/expanded with a Chinese dictionary\n",
    "def rm_nonwords_bitup(ntup): #remove except_bigrams\n",
    "    cp_ntup = ntup.copy() #copy preserves the original ntup, making code edits easier\n",
    "    for tp in cp_ntup[:]:\n",
    "        if tp[0][0] in TM_top_unigram or tp[0][1] in TM_top_unigram: #any bigrams that has any unigrams from TM_top_unigram; note: there is ambiguity in some of these bigrams, e.g., 一起 can mean \"together\" or an incident counter like in 一起 fire/car crash... in which the bigram is actually a phrase, but considering this is a spoken corpus, the colloquial meaning are favored here\n",
    "            if tp[0] in TM_except_bigrams: #exclude compound words that are legitimate words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp) #remove the ngram tuple if it has one of the char in TM_except_bigrams\n",
    "        else:\n",
    "            continue\n",
    "    return cp_ntup\n",
    "TM_bitup = rm_nonwords_bitup(TM_sort_bitup) #bigram tuples with possible phrases/nonwords removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e36af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nonword or phrase trigrams\n",
    "TM_top_bigram = [tp[0] for tp in TM_bitup[:50]] #top 50 bigrams are mostly real words\n",
    "\n",
    "def overlap_unigram(trigram, TM_top_unigram): #if more than 1 TM_top_unigram in the trigram, mark these trigrams to later reject them as these are likely nonwords\n",
    "    count = 0\n",
    "    for char in trigram:\n",
    "        if char in TM_top_unigram:\n",
    "            count += 1\n",
    "    if count > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "TM_except_bigrams = [\"為什麼\", \"對不起\", \"怎麼樣\"]\n",
    "def rm_nonwords_tritup(ntup): #remove trigrams that has TM_top_bigram--most likely nonwords or phrases\n",
    "    cp_ntup = ntup.copy()\n",
    "    for tp in cp_ntup[:]:\n",
    "        if tp[0][:2] in TM_top_bigram or tp[0][1:3] in TM_top_bigram:\n",
    "            if tp[0] in TM_except_bigrams: #exclude exception(s) that are legitimate real words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp)\n",
    "        elif overlap_unigram(tp[0], TM_top_unigram): #trigrams containing more than one common unigrams are likely nonwords/phrases\n",
    "            if tp[0] == \"有沒有\" or tp[0] == \"是不是\": #exclude exception(s) that are legitimate real words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp)\n",
    "        else:\n",
    "            continue\n",
    "    return cp_ntup\n",
    "TM_tritup = rm_nonwords_tritup(TM_sort_tritup) #trigram tuples with possible phrases/nonwords removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove overlapping higher gram counts to avoid inflated counts\n",
    "sort_tup_list = [TM_sort_unitup, TM_bitup, TM_tritup, TM_sort_fourtup] \n",
    "def correct_sort_ntup(n, cutoff): # n<3\n",
    "    sort_ntup = sort_tup_list[n-1] #sorted (n-gram, frequency)\n",
    "    sort_n1tup = sort_tup_list[n] #sorted (n+1-gram, frequency)\n",
    "    correct_ndict = {} \n",
    "    for ngram, value in sort_ntup:\n",
    "        correct_ndict[ngram] = value #collect ngram texts and freq into dictionary for easier frequency editing\n",
    "        for n1gram, vl in sort_n1tup[:int(len(sort_tup_list[n])*0.0025)]: #if the concurrent ngram in most frequent n+1gram, this needs to be deducted to avoid inflated counts, eg., \"什麼(what)\" in \"為什麼(why)\"\n",
    "            if ngram in n1gram:\n",
    "                correct_ndict[ngram] -= vl\n",
    "    correct_sort_ntup = sort_dict2tup(correct_ndict, cutoff) #sort the new dictionary results after the correction\n",
    "    return correct_sort_ntup\n",
    "x = 0.005 #adjust for most frequent ___ words. Here I decided to use 0.5% after reviewing different criteria. It was felt 0.5% preserves the most real words and yields the least non-words/phrases\n",
    "TM_top_tritup = TM_tritup[:int(len(TM_tritup)*x)]\n",
    "TM_top_bitup = correct_sort_ntup(2, int(len(TM_bidict)*x))\n",
    "sort_tup_list.pop(1)\n",
    "sort_tup_list.append(TM_top_bitup)\n",
    "#replace the old n-tuple with the corrected one. This is only meaningful for uni-tuple counts\n",
    "TM_top_unitup = correct_sort_ntup(1, int(len(TM_unidict)*(20*x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54526e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save unique ngrams in txt files\n",
    "with open(\"TM_trigram.txt\", \"w\") as f:\n",
    "    for tup in TM_top_tritup:\n",
    "        f.write(tup[0] + \"\\n\")\n",
    "with open(\"TM_bigram.txt\", \"w\") as f:\n",
    "    for tup in TM_top_bitup:\n",
    "        f.write(tup[0] + \"\\n\")\n",
    "with open(\"TM_unigram.txt\", \"w\") as f:\n",
    "    for tup in TM_top_unitup:\n",
    "        f.write(tup[0] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbc27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read unique CM grams in traditional Chinese and break into list of strings\n",
    "with open(\"CM_trigram.txt\", \"r\") as f:\n",
    "    CM_trigram_txt = chinese_converter.to_traditional(f.read())\n",
    "with open(\"CM_bigram.txt\", \"r\") as f:\n",
    "    CM_bigram_txt = chinese_converter.to_traditional(f.read())\n",
    "with open(\"CM_unigram.txt\", \"r\") as f:\n",
    "    CM_unigram_txt = chinese_converter.to_traditional(f.read())\n",
    "\n",
    "CM_trigram = CM_trigram_txt.split(\"\\n\")[:-1]\n",
    "CM_bigram = CM_bigram_txt.split(\"\\n\")[:-1]\n",
    "CM_unigram = CM_unigram_txt.split(\"\\n\")[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq CM frequent words\n",
    "def uniq_gram_count(n, CM_ngram, cutoff): #\n",
    "    TM_ntup_lst = [TM_sort_unitup, TM_bitup, TM_tritup] #(almost) all n-grams in TM\n",
    "    uniq_CM = []\n",
    "    uniq_CM_count = 0\n",
    "    TM_frequent_ngram = []\n",
    "    for g, f in TM_ntup_lst[n-1][:int(cutoff*len(TM_ntup_lst[n-1]))]:\n",
    "        TM_frequent_ngram.append(g) #create a list of n-grams in TM by cutoff proportion (0<=cutoff<=1)\n",
    "    for ng in CM_ngram:\n",
    "        if ng not in TM_frequent_ngram:\n",
    "            uniq_CM.append(ng) #if an n-gram does not exist in TM_frequent_ngram, then treat it as an unique CM n-gram and add to uniq_CM\n",
    "            uniq_CM_count += 1 #track how many unique n-grams identified in the end\n",
    "    return uniq_CM, uniq_CM_count\n",
    "\n",
    "c = 0.8 #0.8 results seem more meaningful than 0.8 or lower\n",
    "print(uniq_gram_count(3, CM_trigram, c))\n",
    "print(uniq_gram_count(2, CM_bigram, c))\n",
    "print(uniq_gram_count(1, CM_unigram, 0.5*c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca34c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chart making\n",
    "\n",
    "\"\"\"TM_sort_tritup\n",
    "with open(\"TM_trigram.csv\", \"w\") as f:\n",
    "    f.write(\"phrase,freq\\n\")\n",
    "    for tgram in TM_sort_tritup:\n",
    "        f.write(tgram[0]+f\",{tgram[1]}\\n\")\n",
    "with open(\"TM_bigram.csv\", \"w\") as f:\n",
    "    f.write(\"phrase,freq\\n\")\n",
    "    for bgram in TM_sort_bitup:\n",
    "        f.write(bgram[0]+f\",{bgram[1]}\\n\")\n",
    "with open(\"TM_unigram.csv\", \"w\") as f:\n",
    "    f.write(\"phrase,freq\\n\")\n",
    "    for ugram in TM_sort_unitup:\n",
    "        f.write(ugram[0]+f\",{ugram[1]}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7000c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence-end particles (collect the last word of each utterance and then sort by frequency)\n",
    "sent_end = {}\n",
    "for utter in processed_corpus:\n",
    "    if utter[-1] in sent_end:\n",
    "        sent_end[utter[-1]] +=1\n",
    "    else:\n",
    "        sent_end[utter[-1]] =1\n",
    "sent_end_tup = []\n",
    "for end in sent_end:\n",
    "    sent_end_tup.append((end, sent_end[end]))\n",
    "\n",
    "sent_end_tup.sort(key = lambda x: x[1], reverse = True)\n",
    "print(sent_end_tup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
