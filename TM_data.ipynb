{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfed9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, chinese_converter, requests\n",
    "from glob import glob\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a4dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text loading\n",
    "start_page = requests.get(\"http://spokentaiwanmandarin.nccu.edu.tw/corpus-data.html\")\n",
    "tree0 = html.fromstring(start_page.content)\n",
    "corpus_pages = tree0.xpath(\"//a[@class='mod-articles-category-title ' and @href]/@href\") #find the link destination\n",
    "\n",
    "def scraper(url, corpus):\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    p_node = tree.xpath(\"//td[@style='width: 770px;']/p\")\n",
    "    #A consistent and unique feature that marks the field of speech transcription texts is the format of the cell. Therefore, use xpath to find the node with an attribute that specifies the target cell. \n",
    "    if p_node:\n",
    "        dialogue = tree.xpath(\"//td[@style='width: 770px;']/p/text()\")\n",
    "    else:\n",
    "        dialogue = tree.xpath(\"//td[@style='width: 770px;']/text()\") #For some reason, the last sample T050 has one missing layer /p. This is to go around the variance.\n",
    "    corpus += dialogue\n",
    "    return\n",
    "\n",
    "total_corpus = []\n",
    "for page in corpus_pages: \n",
    "    scraper(\"http://spokentaiwanmandarin.nccu.edu.tw\"+page, total_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad49bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving original text locally for future use if needed\n",
    "with open(\"NCCU_spoken_mandarin_corpus.txt\", \"w\") as f: #saving corpus to local file\n",
    "    for sent in total_corpus:\n",
    "        f.write(sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1ec16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process corpus texts\n",
    "#\"hon\" may be used as sentence end particles and is therefore preserved\n",
    "processed_corpus = []\n",
    "extend_count = 0 #syllable extension count--save for future projects: may be related to TM dialect prosody pattern\n",
    "LN_lst = [] #foreign language use to track on the side: any super high fx that can match up to the top 500 mandarin char/word?\n",
    "\n",
    "for turn in total_corpus:\n",
    "    de_unicode = unicodedata.normalize(\"NFKD\", turn)\n",
    "    de_zero = re.sub(r\"\\(0?\\)\", \"\", de_unicode) #resolve byte difference issues in chinese characters\n",
    "    if re.search(r\"=+\", de_zero):\n",
    "        extend_count += len(re.findall(r\"=+\", de_zero))\n",
    "    de_extend = re.sub(r\"=+\", \"\", de_zero) #remove syllable extension marks\n",
    "    if re.search(r\"<L[1-6].*L[1-6]>\", de_extend):\n",
    "        code_switch = re.findall(r\"< ?L ?[1-6].*(?:L ?[1-6] ?>)\", de_extend)\n",
    "        LN_lst += code_switch\n",
    "    elif re.search(r\"[a-zA-Z ]+L ?[1-6] ?>\", de_extend):\n",
    "        code_switch = re.findall(r\"[a-zA-Z ]+L ?[1-6] ?>\", de_extend)\n",
    "        LN_lst += code_switch\n",
    "    de_Ln = re.sub(r\"(?:< ?L ?[1-6].*(?:L ?[1-6] ?>)?)|(?:[a-zA-Z ]+L ?[1-6] ?>)\", \"\", de_extend) #remove code switching marks\n",
    "    delaugh = re.sub(r\"[@＠]+\", \" \", de_Ln) #remove laughing marks\n",
    "    de_bracket = re.sub(r\"[\\[\\]]\", \"\", delaugh) #remove overlapping utterance marks\n",
    "    de_parenth = re.sub(r\"\\(.*\\)\", \"\", de_bracket)\n",
    "    de_exclaim = re.sub(r\"uh|huh|um|hm|mhm|TSK\", \" \", de_parenth) #may be used as sentence separater #remove nonspeech sounds\n",
    "    de_X = re.sub(r\"< ?[xX]|[xX] ?>\", \"\", de_exclaim) #remove unclear char marks\n",
    "    de_3dots = re.sub(r\"…\", r\"..\", de_X) #resolve another byte difference\n",
    "    shi_variant = re.sub(r\"甚\", \"什\", de_3dots)\n",
    "    ta_variant = re.sub(r\"她\", \"他\", shi_variant)\n",
    "    ni_variant = re.sub(r\"妳\", \"你\", ta_variant) #the last 3 to treat chinese character variants as the same char \n",
    "    sent_marker = re.sub(r\"\\.\\.\\.?| \", \",\", ni_variant) #turn pauses into \",\"\n",
    "    sent_lst = sent_marker.split(\",\") #split text by \",\" into utterances\n",
    "\n",
    "    for sent in sent_lst:\n",
    "        if re.search(r\"[\\u4e00-\\u9fff]+\", sent):\n",
    "            processed_corpus.append(sent) #remove utterances w/o any Chinese characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e525f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving raw text locally for future use if needed\n",
    "with open(\"processed_NCCU_corpus.txt\", \"w\") as f: #saving corpus to local file\n",
    "    for sent in processed_corpus:\n",
    "        f.write(sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28982c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114925 51605 2873\n"
     ]
    }
   ],
   "source": [
    "#making dictionary of {ngrams:frequency}\n",
    "def n_dict(n, corpus):\n",
    "    n_dict = {}\n",
    "    for sent in corpus:\n",
    "        sent_len = len(sent) \n",
    "        if sent_len >= n: #skip sentences shorter than n\n",
    "            for num in range(sent_len):\n",
    "                if num+n <= sent_len: #as long as the current n-gram reading window does not exceed the sentence length:\n",
    "                    n_gram = sent[num:num+n] #for each char in the utterance, collect n chars starting from the current char to make n-gram\n",
    "                    if n>2 and n_gram == sent[num]*n: #do not collect an ngram that has 3 or more consecutively repeated chars as these are often interjections\n",
    "                        continue\n",
    "                    else:\n",
    "                        if n_gram in n_dict:\n",
    "                            n_dict[n_gram] +=1\n",
    "                        else:\n",
    "                            n_dict[n_gram] =1\n",
    "                else:\n",
    "                    continue\n",
    "    return n_dict\n",
    "#TM ngram dictionary of {ngram:frequency}:\n",
    "TM_fourdict = n_dict(4, processed_corpus)\n",
    "TM_tridict = n_dict(3, processed_corpus) \n",
    "TM_bidict = n_dict(2, processed_corpus)\n",
    "TM_unidict = n_dict(1, processed_corpus)\n",
    "print(len(TM_tridict), len(TM_bidict), len(TM_unidict)) #n-gram type counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dict by frequnecy into tuples\n",
    "def sort_dict2tup(ndict, cutoff):\n",
    "    tup_lst = []\n",
    "    for key in ndict:\n",
    "        if ndict[key] < 5: #skip extremely low frequency ngrams\n",
    "            continue\n",
    "        else:\n",
    "            tup_lst.append((key,ndict[key])) #turn dictionary keys & values into tuple as dictionaries can not be sorted\n",
    "    tup_lst.sort(key = lambda x: x[1], reverse = True) #order list from highest to lowest freqency\n",
    "    return tup_lst[:cutoff] #cut off n-grams after xth item (x=cutoff)\n",
    "\n",
    "#sorted ngram tuples\n",
    "TM_sort_fourtup = sort_dict2tup(TM_fourdict, len(TM_fourdict)) #decided not to use this information in the end\n",
    "TM_sort_tritup = sort_dict2tup(TM_tridict, len(TM_tridict))\n",
    "TM_sort_bitup = sort_dict2tup(TM_bidict, len(TM_bidict))\n",
    "TM_sort_unitup = sort_dict2tup(TM_unidict, len(TM_unidict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a54ccc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def freq(ntup): #counting total frequency for later use\\n    freq = 0\\n    for ngram, f in ntup:\\n        freq += f\\n    return freq\\nprint(freq(TM_sort_tritup))\\nprint(freq(TM_sort_bitup))\\nprint(freq(TM_sort_unitup))'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def freq(ntup): #counting total frequency for later use\n",
    "    freq = 0\n",
    "    for ngram, f in ntup:\n",
    "        freq += f\n",
    "    return freq\n",
    "print(freq(TM_sort_tritup))\n",
    "print(freq(TM_sort_bitup))\n",
    "print(freq(TM_sort_unitup))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2859ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nonword or phrase bigrams\n",
    "TM_top_unigram = ['是', '就', '我', '他', '的', '那', '有', '不'] #most frequent unigrams that are almost guarantee to be stand-alone real words; [tp[0] for tp in TM_sort_unitup[:7]]\n",
    "TM_except_bigrams = [\"可是\", \"然後\", \"還是\", \"什麼\"] #bigrams that contains TM_top_unigram--most likely non words or phrases. This can later be compiled/expanded with a Chinese dictionary\n",
    "def rm_nonwords_bitup(ntup): #remove except_bigrams\n",
    "    cp_ntup = ntup.copy() #copy preserves the original ntup, making code edits easier\n",
    "    for tp in cp_ntup[:]:\n",
    "        if tp[0][0] in TM_top_unigram or tp[0][1] in TM_top_unigram: #any bigrams that has any unigrams from TM_top_unigram; note: there is ambiguity in some of these bigrams, e.g., 一起 can mean \"together\" or an incident counter like in 一起 fire/car crash... in which the bigram is actually a phrase, but considering this is a spoken corpus, the colloquial meaning are favored here\n",
    "            if tp[0] in TM_except_bigrams: #exclude compound words that are legitimate words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp) #remove the ngram tuple if it has one of the char in TM_except_bigrams\n",
    "        else:\n",
    "            continue\n",
    "    return cp_ntup\n",
    "TM_bitup = rm_nonwords_bitup(TM_sort_bitup) #bigram tuples with possible phrases/nonwords removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163e0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nonword or phrase trigrams\n",
    "TM_top_bigram = [tp[0] for tp in TM_bitup[:50]] #top 50 bigrams are mostly real words\n",
    "#['然後', '什麼', '覺得', '對啊', '因為', '可是', '這樣', '知道', '他們', '一個', '時候', '所以', '可以', '可能', '對對', '不知', '好像', '不會', '現在', 'XX', '怎麼', '應該', '其實', '而且', '比較', '自己', '還是', '這個', '如果', '為什', '你們', '樣子', '東西', '一直', '不要', '也不', '很多', '還有', '個人', '已經', '反正', '想說', '感覺', '後來', '老師', '嗯嗯', '想要', '之後', '一下']\n",
    "\n",
    "def overlap_unigram(trigram, TM_top_unigram): #if more than 1 TM_top_unigram in the trigram, mark these trigrams to later reject them as these are likely nonwords\n",
    "    count = 0\n",
    "    for char in trigram:\n",
    "        if char in TM_top_unigram:\n",
    "            count += 1\n",
    "    if count > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "TM_except_trigrams = [\"為什麼\", \"對不起\", \"怎麼樣\"]\n",
    "def rm_nonwords_tritup(ntup): #remove trigrams that has TM_top_bigram--most likely nonwords or phrases\n",
    "    cp_ntup = ntup.copy()\n",
    "    for tp in cp_ntup[:]:\n",
    "        if tp[0][:2] in TM_top_bigram or tp[0][1:3] in TM_top_bigram:\n",
    "            if tp[0] in TM_except_trigrams: #exclude exception(s) that are legitimate real words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp)\n",
    "        elif overlap_unigram(tp[0], TM_top_unigram): #trigrams containing more than one common unigrams are likely nonwords/phrases\n",
    "            if tp[0] == \"有沒有\" or tp[0] == \"是不是\": #exclude exception(s) that are legitimate real words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp)\n",
    "        else:\n",
    "            continue\n",
    "    return cp_ntup\n",
    "TM_tritup = rm_nonwords_tritup(TM_sort_tritup) #trigram tuples with possible phrases/nonwords removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35bd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove overlapping higher gram counts to avoid inflated counts\n",
    "sort_tup_list = [TM_sort_unitup, TM_bitup, TM_tritup, TM_sort_fourtup] \n",
    "def correct_sort_ntup(n, cutoff): # n<3\n",
    "    sort_ntup = sort_tup_list[n-1] #sorted (n-gram, frequency)\n",
    "    sort_n1tup = sort_tup_list[n] #sorted (n+1-gram, frequency)\n",
    "    correct_ndict = {} \n",
    "    for ngram, value in sort_ntup:\n",
    "        correct_ndict[ngram] = value #collect ngram texts and freq into dictionary for easier frequency editing\n",
    "        for n1gram, vl in sort_n1tup[:int(len(sort_tup_list[n])*0.0025)]: #if the concurrent ngram in most frequent n+1gram, this needs to be deducted to avoid inflated counts, eg., \"什麼(what)\" in \"為什麼(why)\"\n",
    "            if ngram in n1gram:\n",
    "                correct_ndict[ngram] -= vl\n",
    "    correct_sort_ntup = sort_dict2tup(correct_ndict, cutoff) #sort the new dictionary results after the correction\n",
    "    return correct_sort_ntup\n",
    "x = 0.02 #adjust for most frequent ___ words. Here I decided to use 2% after reviewing different criteria. At this criteria it yields around 50x occurrence of the n-grams and seems to preserve the most real words and return the least non-words/phrases\n",
    "TM_top_tritup = TM_tritup[:int(len(TM_tritup)*x)]\n",
    "TM_top_bitup = correct_sort_ntup(2, int(len(TM_bidict)*0.375*x))\n",
    "sort_tup_list.pop(1) #replace the old n-tuple with the corrected one. This is only meaningful for uni-tuple counts\n",
    "sort_tup_list.append(TM_top_bitup)\n",
    "TM_top_unitup = correct_sort_ntup(1, int(len(TM_unidict)*10*x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea32e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save unique ngrams in txt files\n",
    "with open(\"TM_trigram.txt\", \"w\") as f:\n",
    "    for tup in TM_top_tritup:\n",
    "        f.write(tup[0] + \"\\n\")\n",
    "with open(\"TM_bigram.txt\", \"w\") as f:\n",
    "    for tup in TM_top_bitup:\n",
    "        f.write(tup[0] + \"\\n\")\n",
    "with open(\"TM_unigram.txt\", \"w\") as f:\n",
    "    for tup in TM_top_unitup:\n",
    "        f.write(tup[0] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ee5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read unique CM grams in traditional Chinese and break into list of strings\n",
    "with open(\"CM_trigram.txt\", \"r\") as f:\n",
    "    CM_trigram_txt = chinese_converter.to_traditional(f.read())\n",
    "with open(\"CM_bigram.txt\", \"r\") as f:\n",
    "    CM_bigram_txt = chinese_converter.to_traditional(f.read())\n",
    "with open(\"CM_unigram.txt\", \"r\") as f:\n",
    "    CM_unigram_txt = chinese_converter.to_traditional(f.read())\n",
    "\n",
    "CM_trigram = CM_trigram_txt.split(\"\\n\")[:-1]\n",
    "CM_bigram = CM_bigram_txt.split(\"\\n\")[:-1]\n",
    "CM_unigram = CM_unigram_txt.split(\"\\n\")[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1a3757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['新西蘭', '挺好的', '比方說', '但是但', '在北京', '肯定是', '在新西', '還挺好', '我之前', '西蘭的', '在國內', '個孩子', '如果你'], 13)\n",
      "(['肯定', '還挺', '明白', '英語', '也挺', '方說', '別特', '房東', '在國', '在北', '漢語', '麼著', '後當', '環境', '正好', '估計', '旅遊', '挺好', '咱們', '者說', '別好', '們倆', '吧然', '月份', '經常', '個孩', '屬於', '經歷'], 28)\n",
      "(['挺', '啥', '唄', '咱', '倆', '呆', '估'], 7)\n"
     ]
    }
   ],
   "source": [
    "#uniq CM frequent words\n",
    "def uniq_gram_count(n, CM_ngram, cutoff): #\n",
    "    TM_ntup_lst = [TM_sort_unitup, TM_bitup, TM_tritup] #(almost) all n-grams in TM\n",
    "    uniq_CM = []\n",
    "    uniq_CM_count = 0\n",
    "    TM_frequent_ngram = []\n",
    "    for g, f in TM_ntup_lst[n-1][:int(cutoff*len(TM_ntup_lst[n-1]))]:\n",
    "        TM_frequent_ngram.append(g) #create a list of n-grams in TM by cutoff proportion (0<=cutoff<=1)\n",
    "    for ng in CM_ngram:\n",
    "        if ng not in TM_frequent_ngram:\n",
    "            uniq_CM.append(ng) #if an n-gram does not exist in TM_frequent_ngram, then treat it as an unique CM n-gram and add to uniq_CM\n",
    "            uniq_CM_count += 1 #track how many unique n-grams identified in the end\n",
    "    return uniq_CM, uniq_CM_count\n",
    "\n",
    "c = 0.9 #0.9 results seem more meaningful than 0.9 or lower\n",
    "print(uniq_gram_count(3, CM_trigram, c))\n",
    "#['新西蘭', '挺好的', '比方說', '但是但', '在北京', '肯定是', '在新西', '還挺好', '我之前', '西蘭的', '在國內', '個孩子', '如果你']\n",
    "print(uniq_gram_count(2, CM_bigram, c))\n",
    "#['肯定', '還挺', '明白', '英語', '也挺', '方說', '別特', '房東', '在國', '在北', '漢語', '麼著', '後當', '環境', '正好', '估計', '旅遊', '挺好', '咱們', '者說', '別好', '們倆', '吧然', '月份', '經常', '個孩', '屬於', '經歷']\n",
    "print(uniq_gram_count(1, CM_unigram, c))\n",
    "#['挺', '啥', '唄', '咱', '倆', '呆', '估']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0d98f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12414\n",
      "8620\n"
     ]
    }
   ],
   "source": [
    "#sentence-final particles (collect the last word of each utterance and then sort by frequency)\n",
    "sent_end = {}\n",
    "for utter in processed_corpus:\n",
    "    if utter[-1] in sent_end: #collect last character in each utterance into sent_end and track the frequency\n",
    "        sent_end[utter[-1]] +=1\n",
    "    else:\n",
    "        sent_end[utter[-1]] =1\n",
    "sent_end_tup = []\n",
    "for end in sent_end: #turn the sent_end dictionary into tuple list \n",
    "    sent_end_tup.append((end, sent_end[end]))\n",
    "\n",
    "sent_end_tup.sort(key = lambda x: x[1], reverse = True) #sort the tuple list\n",
    "freq_SFP = ['啊','喔','嗎','了','吧','啦','耶','欸','嘛','呀','呢','哦','噢','吶'] #extract the most frequent SFP in TM\n",
    "common_SFP = ['啊','嗎','了','吧','嘛','呀','呢','哦'] #common SFPs only\n",
    "def token_count(tuplst, lst): \n",
    "    count = 0\n",
    "    for tp in tuplst:\n",
    "        if tp[0] in lst:\n",
    "            count += tp[1]\n",
    "    return count\n",
    "print(token_count(sent_end_tup, freq_SFP))\n",
    "print(token_count(sent_end_tup, common_SFP)) #token frequency for the 8 common SFPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e994259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2366\n"
     ]
    }
   ],
   "source": [
    "common_SFP_omit_le = ['啊','嗎','吧','嘛','呀','呢','哦']\n",
    "def SFP_count(chi_char):\n",
    "    SFP_count = 0\n",
    "    for utt in processed_corpus:\n",
    "        for char in utt:\n",
    "            if char == chi_char:\n",
    "                SFP_count += 1\n",
    "    return SFP_count\n",
    "print(SFP_count(\"了\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1084cc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.028853727973602437, 9304, 322454)\n"
     ]
    }
   ],
   "source": [
    "def prob_SFP(corpus, SFP_lst):\n",
    "    count = 0\n",
    "    corpus_len = 0\n",
    "    for utt in corpus:\n",
    "        corpus_len += len(utt)\n",
    "        for char in utt:\n",
    "            if char in SFP_lst:\n",
    "                count += 1\n",
    "    return count/(corpus_len-SFP_count(\"了\")), count, corpus_len-SFP_count(\"了\")\n",
    "print(prob_SFP(processed_corpus, common_SFP_omit_le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37cd8b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TM_sort_tritup\\nwith open(\"TM_trigram.csv\", \"w\") as f:\\n    f.write(\"phrase,freq\\n\")\\n    for tgram in TM_sort_tritup:\\n        f.write(tgram[0]+f\",{tgram[1]}\\n\")\\nwith open(\"TM_bigram.csv\", \"w\") as f:\\n    f.write(\"phrase,freq\\n\")\\n    for bgram in TM_sort_bitup:\\n        f.write(bgram[0]+f\",{bgram[1]}\\n\")\\nwith open(\"TM_unigram.csv\", \"w\") as f:\\n    f.write(\"phrase,freq\\n\")\\n    for ugram in TM_sort_unitup:\\n        f.write(ugram[0]+f\",{ugram[1]}\\n\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chart making only\n",
    "\n",
    "\"\"\"TM_sort_tritup\n",
    "with open(\"TM_trigram.csv\", \"w\") as f:\n",
    "    f.write(\"phrase,freq\\n\")\n",
    "    for tgram in TM_sort_tritup:\n",
    "        f.write(tgram[0]+f\",{tgram[1]}\\n\")\n",
    "with open(\"TM_bigram.csv\", \"w\") as f:\n",
    "    f.write(\"phrase,freq\\n\")\n",
    "    for bgram in TM_sort_bitup:\n",
    "        f.write(bgram[0]+f\",{bgram[1]}\\n\")\n",
    "with open(\"TM_unigram.csv\", \"w\") as f:\n",
    "    f.write(\"phrase,freq\\n\")\n",
    "    for ugram in TM_sort_unitup:\n",
    "        f.write(ugram[0]+f\",{ugram[1]}\\n\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
