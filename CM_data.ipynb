{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8144d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, chinese_converter\n",
    "from glob import glob\n",
    "from lxml import etree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d836187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text loading\n",
    "corpus_files = glob(\"The-spoken-L1-corpus-main/L1-L1 transcripts/*.txt\")\n",
    "CM_txt = \"\"\n",
    "for file in corpus_files:\n",
    "    with open(file, \"r\", encoding='utf-8-sig') as f:\n",
    "        CM_txt += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b412f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pre-process corpus texts\n",
    "processed_corpus0 = []\n",
    "de_space = re.sub(\" \", \"\\n\", CM_txt)\n",
    "de_mark = re.sub(r\"<.+> ?\", \"\", de_space) #remove codes in < > as it's coded for speaker ID, depersonalizing naming, and marks of uncertain transcripts\n",
    "ta_variant = re.sub(r\"她\", \"他\", de_mark) #\"she\" is pronounced the same as \"he\"; therefore, here all \"she\" are turned into \"he\"\n",
    "TA_char = re.sub(r\"TA\", \"他\", ta_variant) #the author of the corpus coded ambiguous he vs she as \"TA\". All \"TA\" turned into \"he\"\n",
    "de_filler = re.sub(r\"(?<![a-zA-Z])eng|erm?(?![a-zA-Z])\", \"\\n\", TA_char) #remove backchanneling, \"eng\", \"er\", \"erm\"\n",
    "de_er2 = re.sub(r\"儿(?!子|童)\", \"\", de_filler) #remove 儿 due to it being mostly for phonemic markin#g\n",
    "sents = de_er2.split(\"\\n\")\n",
    "for sent in sents:\n",
    "    if re.search(r\"[\\u4e00-\\u9fff]+\", sent): \n",
    "        processed_corpus0.append(sent)\n",
    "#len(processed_corpus): 8483\n",
    "#repeated bigram: 2114; repeatd trigram: 283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd0ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = []\n",
    "for utt in processed_corpus0: #reduce repeated characters to one\n",
    "    rep = re.findall(r\"(.)\\1+?\", utt)\n",
    "    if len(rep) == 1: #only one rep 1598x vs more than one rep 523x\n",
    "        de_rep = re.sub(r\"(.)\\1+?\", rep[0], utt)\n",
    "        processed_corpus.append(de_rep)\n",
    "    else:\n",
    "        processed_corpus.append(utt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394dc863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104564 41635 2312\n"
     ]
    }
   ],
   "source": [
    "#ngrams\n",
    "def n_dict(n, corpus):\n",
    "    n_dict = {}\n",
    "    for sent in corpus:\n",
    "        sent_len = len(sent)\n",
    "        if sent_len >= n:\n",
    "            for num in range(sent_len):\n",
    "                if num+n <= sent_len:\n",
    "                    n_gram = sent[num:num+n]\n",
    "                    if n>1 and n_gram == sent[num]*n:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if n_gram in n_dict:\n",
    "                            n_dict[n_gram] +=1\n",
    "                        else:\n",
    "                            n_dict[n_gram] =1\n",
    "                else:\n",
    "                    continue\n",
    "    return n_dict\n",
    "CM_fourdict = n_dict(4, processed_corpus)\n",
    "CM_tridict = n_dict(3, processed_corpus)\n",
    "CM_bidict = n_dict(2, processed_corpus)\n",
    "CM_unidict = n_dict(1, processed_corpus)\n",
    "print(len(CM_tridict), len(CM_bidict), len(CM_unidict)) #n-gram type count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568bd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dict by frequnecy into tuples\n",
    "def sort_dict2tup(ndict, cutoff):\n",
    "    tup_lst = []\n",
    "    for key in ndict:\n",
    "        if ndict[key] < 5: #skip extremely low frequency ngrams\n",
    "            continue\n",
    "        else:\n",
    "            tup_lst.append((key,ndict[key]))\n",
    "    tup_lst.sort(key = lambda x: x[1], reverse = True)\n",
    "    return tup_lst[:cutoff+1]\n",
    "\n",
    "#CM sorted tuples\n",
    "CM_sort_fourtup = sort_dict2tup(CM_fourdict, len(CM_fourdict))\n",
    "CM_sort_tritup = sort_dict2tup(CM_tridict, len(CM_tridict))\n",
    "CM_sort_bitup = sort_dict2tup(CM_bidict, len(CM_bidict))\n",
    "CM_sort_unitup = sort_dict2tup(CM_unidict, len(CM_unidict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1613716f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove nonword or phrase bigrams\n",
    "CM_top_unigram = ['是', '就', '的', '我', '那', '有', '他', '不'] \n",
    "#['是', '就', '的', '我', '个', '那', '后']\n",
    "CM_except_bigrams = [\"可是\", \"然后\", \"还是\", \"什么\"]\n",
    "\n",
    "def rm_nonwords_bitup(ntup): #remove bigrams that contains CM_top_unigram--most likely non words or phrases\n",
    "    cp_ntup = ntup.copy() #copy preserves the original ntup, making code edits easier\n",
    "    for tp in cp_ntup[:]:\n",
    "        if tp[0][0] in CM_top_unigram or tp[0][1] in CM_top_unigram: #any bigrams that has any unigrams from CM_top_unigram\n",
    "            if tp[0] in CM_except_bigrams:\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp)\n",
    "        else:\n",
    "            continue\n",
    "    return cp_ntup\n",
    "CM_bitup = rm_nonwords_bitup(CM_sort_bitup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6a019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nonword or phrase trigrams\n",
    "CM_top_bigram = [tp[0] for tp in CM_bitup[:40]] #TM selected top 50 bigrams. Overall CM bigram inventory is ~80% of TM bigram inventory. Therefore, CM selects top 50*80%=40 bigrams\n",
    "#['然后', '这个', '什么', '觉得', '一个', '因为', '可能', '时候', '特别', '其实', '所以', '比较', '还是', '这种', '自己', '这样', '一些', '知道', '现在', '怎么', '当时', '感觉', '可以', '反正', '了一', '很多', '之后', '或者', '而且', '也不', '应该', '好像', '一下', '这边', '你们', '地方', '后来', '时间', '个人', '去了', '东西']\n",
    "\n",
    "def overlap_unigram(trigram, CM_top_unigram): #if more than 1 CM_top_unigram in the trigram, mark these trigrams to later reject them as these are likely nonwords\n",
    "    count = 0\n",
    "    for char in trigram:\n",
    "        if char in CM_top_unigram:\n",
    "            count += 1\n",
    "    if count > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "CM_except_bigrams = [\"为什么\", \"对不起\", \"新西兰\", \"怎么样\"]\n",
    "def rm_nonwords_tritup(ntup): #remove trigrams that has CM_top_bigram--most likely nonwords or phrases\n",
    "    cp_ntup = ntup.copy()\n",
    "    for tp in cp_ntup[:]:\n",
    "        if tp[0][:2] in CM_top_bigram or tp[0][1:3] in CM_top_bigram:\n",
    "            if tp[0] in CM_except_bigrams: #exclude exception(s) that are legitimate real words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp)\n",
    "        elif overlap_unigram(tp[0], CM_top_unigram): #trigrams containing more than one common unigrams are likely nonwords/phrases\n",
    "            if tp[0] == \"有没有\" or tp[0] == \"是不是\": #exclude exception(s) that are legitimate real words\n",
    "                continue\n",
    "            else:\n",
    "                cp_ntup.remove(tp)\n",
    "        else:\n",
    "            continue\n",
    "    return cp_ntup\n",
    "CM_tritup = rm_nonwords_tritup(CM_sort_tritup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef64bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove overlapping higher gram counts to avoid inflated counts\n",
    "sort_tup_list = [CM_sort_unitup, CM_bitup, CM_tritup, CM_sort_fourtup]\n",
    "def correct_sort_ntup(n, cutoff): # n<3\n",
    "    sort_ntup = sort_tup_list[n-1]\n",
    "    sort_n1tup = sort_tup_list[n]\n",
    "    correct_ndict = {} \n",
    "    for ngram, value in sort_ntup:\n",
    "        correct_ndict[ngram] = value #collect ngram texts and freq into dictionary for easier frequency editing\n",
    "        for n1gram, vl in sort_n1tup[:int(len(sort_tup_list[n])*0.0025)]: #if the concurrent ngram in most frequent n+1gram, this needs to be deducted to avoid inflated counts, eg., \"什麼(what)\" in \"為什麼(why)\"\n",
    "            if ngram in n1gram:\n",
    "                correct_ndict[ngram] -= vl\n",
    "    correct_sort_ntup = sort_dict2tup(correct_ndict, cutoff) #sort the new dictionary results after the correction\n",
    "    return correct_sort_ntup\n",
    "x = 0.02\n",
    "CM_top_tritup = CM_tritup[:int(len(CM_tritup)*x)]\n",
    "CM_top_bitup = correct_sort_ntup(2, int(len(CM_bidict)*0.375*x))\n",
    "sort_tup_list.pop(1)\n",
    "sort_tup_list.append(CM_top_bitup)\n",
    "#replace the old n-tuple with the corrected one. This is only meaningful for uni-tuple counts\n",
    "CM_top_unitup = correct_sort_ntup(1, int(len(CM_unidict)*(10*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d51e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the ngrams (text only) into txt file\n",
    "with open(\"CM_trigram.txt\", \"w\") as f:\n",
    "    for tup in CM_top_tritup:\n",
    "        f.write(tup[0] + \"\\n\")\n",
    "with open(\"CM_bigram.txt\", \"w\") as f:\n",
    "    for tup in CM_top_bitup:\n",
    "        f.write(tup[0] + \"\\n\")\n",
    "with open(\"CM_unigram.txt\", \"w\") as f:\n",
    "    for tup in CM_top_unitup:\n",
    "        f.write(tup[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb066af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read unique CM grams in simplified Chinese and break into list of strings\n",
    "with open(\"TM_trigram.txt\", \"r\") as f:\n",
    "    TM_trigram_txt = chinese_converter.to_simplified(f.read())\n",
    "with open(\"TM_bigram.txt\", \"r\") as f:\n",
    "    TM_bigram_txt = chinese_converter.to_simplified(f.read())\n",
    "with open(\"TM_unigram.txt\", \"r\") as f:\n",
    "    TM_unigram_txt = chinese_converter.to_simplified(f.read())\n",
    "\n",
    "TM_trigram = TM_trigram_txt.split(\"\\n\")[:-1]\n",
    "TM_bigram = TM_bigram_txt.split(\"\\n\")[:-1]\n",
    "TM_unigram = TM_unigram_txt.split(\"\\n\")[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "906def83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['真的喔', '对不对', '跟你讲', '好不好', '跟他讲', '个礼拜', '跟我讲', '是怎样', '在干嘛', '譬如说', '没有啦', '超好笑', '很好笑', '我同学'], 14)\n",
      "(['对对', '嗯嗯', '喔喔', '好笑', '讲说', '哪里', '妈妈', '好啦', '喔对', '夸张', '好好', '很像', '重点', '超好', '讲话', '很久', '刚好', '哪一', '台北', '可怕', '看看', '看起', '譬如', '在干', '恐怖', '后后', '国中', '么啊', '通常', '等一', '韩国', '刚刚', '头发', '很可', '部都', '爸爸', '原本', '可爱', '法律'], 39)\n",
      "(['喔', '嗯', '耶', '-', '婆', '夸', '姊', '姑', '吵', '噢', '韩', '.', '烂', '恐', '夜', '李', '爆', '鬼', '譬', '帅', '怖', '赚', '扣', '赛', '1', '鞋', '薪', '群'], 28)\n"
     ]
    }
   ],
   "source": [
    "#uniq TM frequent words\n",
    "def uniq_gram_count(n, TM_ngram, cutoff):\n",
    "    CM_ntup_lst = [CM_sort_unitup, CM_bitup, CM_tritup]\n",
    "    uniq_TM = []\n",
    "    uniq_TM_count = 0\n",
    "    CM_frequent_ngram = []\n",
    "    for g, f in CM_ntup_lst[n-1][:int(cutoff*len(CM_ntup_lst[n-1]))]:\n",
    "        CM_frequent_ngram.append(g)\n",
    "    for ng in TM_ngram:\n",
    "        if ng not in CM_frequent_ngram:\n",
    "            if re.search(r\"[a-zA-Z]+\", ng):\n",
    "                continue\n",
    "            else:\n",
    "                uniq_TM.append(ng)\n",
    "                uniq_TM_count += 1\n",
    "    return uniq_TM, uniq_TM_count\n",
    "\n",
    "c = 0.9\n",
    "print(uniq_gram_count(3, TM_trigram, c))\n",
    "print(uniq_gram_count(2, TM_bigram, c))\n",
    "print(uniq_gram_count(1, TM_unigram, c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08bf0b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def to_trad_chin(lst): #translate to TM for documentation purpose\\n    convt_lst = []\\n    for word in lst:\\n        convt_lst.append(chinese_converter.to_traditional(word))\\n    return convt_lst\\nt = uniq_gram_count(3, TM_trigram, c)[0]\\nb = uniq_gram_count(2, TM_bigram, c)[0]\\nu = uniq_gram_count(1, TM_unigram, c)[0]\\nprint(to_trad_chin(t))\\nprint(to_trad_chin(b))\\nprint(to_trad_chin(u))'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def to_trad_chin(lst): #translate to TM for documentation purpose\n",
    "    convt_lst = []\n",
    "    for word in lst:\n",
    "        convt_lst.append(chinese_converter.to_traditional(word))\n",
    "    return convt_lst\n",
    "t = uniq_gram_count(3, TM_trigram, c)[0]\n",
    "b = uniq_gram_count(2, TM_bigram, c)[0]\n",
    "u = uniq_gram_count(1, TM_unigram, c)[0]\n",
    "print(to_trad_chin(t))\n",
    "print(to_trad_chin(b))\n",
    "print(to_trad_chin(u))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07722c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260\n",
      "2229\n"
     ]
    }
   ],
   "source": [
    "sent_end = {}\n",
    "for utter in processed_corpus:\n",
    "    if utter[-1] in sent_end:\n",
    "        sent_end[utter[-1]] +=1\n",
    "    else:\n",
    "        sent_end[utter[-1]] =1\n",
    "sent_end_tup = []\n",
    "for end in sent_end:\n",
    "    sent_end_tup.append((end, sent_end[end]))\n",
    "\n",
    "sent_end_tup.sort(key = lambda x: x[1], reverse = True)\n",
    "freq_SFP = ['啊','吗','了','吧','嘛','呀','呢','哦','呗']\n",
    "common_SFP = ['啊','吗','了','吧','嘛','呀','呢','哦']\n",
    "def token_count(tuplst, lst): \n",
    "    count = 0\n",
    "    for tp in tuplst:\n",
    "        if tp[0] in lst:\n",
    "            count += tp[1]\n",
    "    return count\n",
    "print(token_count(sent_end_tup, freq_SFP))\n",
    "print(token_count(sent_end_tup, common_SFP)) #token frequency for the 8 common SFPs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08d890e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2806\n"
     ]
    }
   ],
   "source": [
    "common_SFP_omit_le = ['啊','吗','吧','嘛','呀','呢','哦']\n",
    "def SFP_count(chi_char):\n",
    "    SFP_count = 0\n",
    "    for utt in processed_corpus:\n",
    "        for char in utt:\n",
    "            if char == chi_char:\n",
    "                SFP_count += 1\n",
    "    return SFP_count\n",
    "print(SFP_count(\"了\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0288344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.02501605681786449, 5375, 214862)\n"
     ]
    }
   ],
   "source": [
    "def prob_SFP(corpus, SFP_lst):\n",
    "    count = 0\n",
    "    corpus_len = 0\n",
    "    for utt in corpus:\n",
    "        corpus_len += len(utt)\n",
    "        for char in utt:\n",
    "            if char in SFP_lst:\n",
    "                count += 1\n",
    "    return count/(corpus_len-SFP_count(\"了\")), count, corpus_len-SFP_count(\"了\")\n",
    "print(prob_SFP(processed_corpus, common_SFP_omit_le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b3d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
